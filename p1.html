<h1 style='margin-top:24.0pt;page-break-after:auto'><a name="_4vlc4jvh776s"></a><b><span
style='font-size:23.0pt;line-height:115%'>The challenge, should you choose to
accept it.</span></b></h1>

<p class=MsoNormal style='margin-top:12.0pt;margin-right:0in;margin-bottom:
12.0pt;margin-left:0in;text-align:justify'>Today aerial autonomous systems for
navigation and control are heavily dependent on<span class=msoIns><ins
cite="mailto:Qi%20Yan" datetime="2022-06-17T21:38"> the</ins></span> robustness
of GNSS reception. Lack of thereof (terrain, weather<span class=msoIns><ins
cite="mailto:Qi%20Yan" datetime="2022-06-17T21:38">,</ins></span> or
adversarial spoofing) can lead <span class=msoIns><ins cite="mailto:Qi%20Yan"
datetime="2022-06-17T21:39">to </ins></span>loss of autonomous system absolute
orientation in the medium to long run and be unsafe for operation on beyond
line of sight missions. Despite significant progress in Computer Vision, most
learning-based approaches target at a single domain and require a dense
database of geo-tagged images to function well. Or at least a calibration set
of real images taken closely from the domain of [1,2] operation. Several
industry attempts are made in the same direction, however, understandably
without <span class=msoIns><ins cite="mailto:Qi%20Yan"
datetime="2022-06-17T21:39">an </ins></span>official benchmark or validation.</p>

<p class=MsoNormal style='margin-top:12.0pt;margin-right:0in;margin-bottom:
12.0pt;margin-left:0in;text-align:justify'><img border=0 width=668 height=160
id=image3.png src="Website%20txt_files/image003.png">We challenge you to prove
your approach can work <b>robustly</b> and in an <b>economically/data scalable
way</b>. To do this by proving you can compute accurate 6D camera poses with
known uncertainty on our challenge validation dataset. To do this by having
only access to:</p>